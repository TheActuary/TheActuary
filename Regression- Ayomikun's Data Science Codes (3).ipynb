{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c154f810",
   "metadata": {},
   "source": [
    "# Data Scientist 0b11111100110's Python Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07207a",
   "metadata": {},
   "source": [
    "# For Data Checking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097dce82",
   "metadata": {},
   "source": [
    "For Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)\n",
    "df = pd.read_csv(other_path, header=None) '''if no headers are there'''\n",
    "df = pd.read_csv('auto.csv', names = headers) '''if you want to set your headers while importing data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdc5bf",
   "metadata": {},
   "source": [
    "For Setting Values to Pandas Columns or Checking Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = headers '''where headers is the variable containing the list of column names'''\n",
    "print (df.columns) '''to call the columns'''\n",
    "df.rename(columns={'old name':'new name', 'old name':'new name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942026c",
   "metadata": {},
   "source": [
    "To Join a Column or Columns to a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, the_column], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be3030",
   "metadata": {},
   "source": [
    "To Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d0fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path) '''path has to be a path in the computer'''\n",
    "df.to_csv(path, index=False) '''use index = False to avoid saving the row indexes to csv file'''\n",
    "df.to_csv('clean_df.csv') '''to save dataset to jupyter homepage'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe0eea",
   "metadata": {},
   "source": [
    "To Check Dataset Qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n",
    "df.describe()\n",
    "df.describe(include = \"all\") '''to include the NaN columns(strings)'''\n",
    "df[[' column 1 ',column 2', 'column 3'] ].describe() '''to get statistics for certain columns'''\n",
    "df.info() '''gets the column data types and also displays the amount of non-null values in columns'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cba2df3",
   "metadata": {},
   "source": [
    "# For Data Wrangling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7ad241",
   "metadata": {},
   "source": [
    "To Replace Missing Values from ? to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df.replace(\"?\", np.nan, inplace = True) '''Note that Numpy first has to be imported'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4425d10",
   "metadata": {},
   "source": [
    "To Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fafd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_sub = housing.drop_duplicates(subset=['Order'])\n",
    "\n",
    "'''to check the duplicated rows by the Order column'''\n",
    "\n",
    "\n",
    "housing.index.is_unique\n",
    "\n",
    "'''for duplicated indexes'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbf972",
   "metadata": {},
   "source": [
    "To Check and Sort Missing Data by Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52833e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = housing.isnull().sum().sort_values(ascending=False)\n",
    "total\n",
    "\n",
    "Alternate Code: \n",
    "    \n",
    "gasoline.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20e9abb",
   "metadata": {},
   "source": [
    "Dropping Missing Values by Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"Lot Frontage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676601f",
   "metadata": {},
   "source": [
    "For Replacing Missing Values With Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e9671",
   "metadata": {},
   "outputs": [],
   "source": [
    "New Code:\n",
    "\n",
    "median = housing[\"Lot Frontage\"].median() #For filling with median\n",
    "median\n",
    "housing[\"Lot Frontage\"].fillna(median, inplace = True)\n",
    "\n",
    "\n",
    "mean = housing[\"Mas Vnr Area\"].mean() #Filling with mean\n",
    "housing[\"Mas Vnr Area\"].fillna(mean, inplace = True)  \n",
    "\n",
    "\n",
    "Old Code:\n",
    "\n",
    "the_average = df[\"column\"].astype(\"float\").mean(axis=0)\n",
    "df[\"column\"].replace(np.nan, the_average, inplace=True)\n",
    "\n",
    "'''astype('float') can be removed if the data type doesnt have to be converted to float'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ff0090",
   "metadata": {},
   "source": [
    "To Replace Missing Values with Some Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea957ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0) #This replaces null values with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aad89c",
   "metadata": {},
   "source": [
    "To Replace with Highest Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column'].value_counts() \n",
    "df[\"column\"].replace(np.nan, \"mostfrequentvalue\", inplace=True)\n",
    "\n",
    "'''value counts shows the unique values. Then the most frequent can be picked to replace the null values'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f340b1",
   "metadata": {},
   "source": [
    "To Drop Entire Rows and Reset Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b6129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"column\"], axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "'''index is reset because the whole thing gets scrambled after dropping rows'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a548c",
   "metadata": {},
   "source": [
    "To Drop Entire Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.drop(\"Lot Frontage\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925c730",
   "metadata": {},
   "source": [
    "Changing Data Types for Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab58e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"column\"]] = df[[\"column\"]].astype(\"float\")\n",
    "\n",
    "'''for a single column'''\n",
    "\n",
    "df[[\"column1\", \"column2\"]] = df[[\"column1\", \"column2\"]].astype(\"int\")\n",
    "\n",
    "'''for multiple columns'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41537770",
   "metadata": {},
   "source": [
    "To Copy a Dataset so the Original can be Maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914abf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = X3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ada40",
   "metadata": {},
   "source": [
    "Replace Values in a Column with Another Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a29a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['species'] = data.species.str.replace('Iris-', '') #replaces Iris with nothing\n",
    "\n",
    "Alternatively:\n",
    "    \n",
    "data['Airline'] = np.where(data['Airline']=='Multiple carriers Premium economy', 'Multiple carriers', data['Airline'])\n",
    "#replaces the first with the second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe38100",
   "metadata": {},
   "source": [
    "Adding Column with Changes to Another Existing Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9b31ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['city-L/100km'] = 235/df[\"city-mpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff09444",
   "metadata": {},
   "source": [
    "Creating a Seaborn Pairplot of Visualization for Particular Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ded58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))\n",
    "\n",
    "#smaller df was a df with certain columns that seemed to have correlational value with SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9da51f",
   "metadata": {},
   "source": [
    "Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02577da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column'] = df['column']/df['column'].max()\n",
    "\n",
    "'''this is the simple feature scaling method'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d26972",
   "metadata": {},
   "source": [
    "Data Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e42cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(df[\"horsepower\"]), max(df[\"horsepower\"]), 4)\n",
    "group_names = ['Low', 'Medium', 'High']\n",
    "df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )\n",
    "\n",
    "df[\"horsepower-binned\"].value_counts()\n",
    "'''value counts is optional'''\n",
    "\n",
    "Alternative Code:\n",
    "\n",
    "data1['dep_timezone'] = pd.cut(data1.Dep_Hour, [0,6,12,18,24], labels=['Night','Morning','Afternoon','Evening'])\n",
    "data1['dep_timezone'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c4f87",
   "metadata": {},
   "source": [
    "Splitting One Column into Two New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['City', 'Province']] = data['GEO'].str.split(',', n=1, expand=True) \n",
    "\n",
    "'''city and province are the new columns to be created. GEO is the column we are splitting'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938f1b6",
   "metadata": {},
   "source": [
    "Splitting a Column into Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d53b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DATE'] = pd.to_datetime(data['DATE'], format='%b-%y') #converts Jan-1979 to figures in datetime\n",
    "data['Month'] = data['DATE'].dt.month_name().str.slice(stop=3)\n",
    "data['Year'] = data['DATE'].dt.year\n",
    "\n",
    "'''%b-%y means it will split into month and year. str.slice = 3 means it will output the first three letters of a month'''\n",
    "\n",
    "data1[\"Dep_Hour\"]= pd.to_datetime(data1['Dep_Time']).dt.hour\n",
    "data1[\"Dep_Min\"]= pd.to_datetime(data1['Dep_Time']).dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e10051",
   "metadata": {},
   "source": [
    "Filtering a Column on One Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110e90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "calgary = data[data['GEO'] == 'Calgary, Alberta'] #filters the GEO column down to only Calgary, Alberta\n",
    "calgary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf6eeb",
   "metadata": {},
   "source": [
    "Filtering on Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710154df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_loc = data[(data['GEO'] == \"Toronto, Ontario\") | (data['GEO'] == \"Edmonton, Alberta\")] \n",
    "mult_loc #the above selects Toronto and Edmonton only. The | sign means 'or'\n",
    "\n",
    "Alternatively, you can use:\n",
    "\n",
    "cities = ['Calgary', 'Toronto', 'Edmonton']\n",
    "CTE = data[data.City.isin(cities)]\n",
    "CTE #the above selects only those cities \n",
    "\n",
    "house_heat_fuel_1990 = data[(data['TYPE'] == \"Household heating fuel\") & (data['Year'] == 1990)] \n",
    "house_heat_fuel_1990 #the above filters on multiple columns\n",
    "\n",
    "exercise2b = data[( data['Year'] <=  1979) | ( data['Year'] ==  2021) & (data['TYPE'] == \"Household heating fuel\") \n",
    "                  & (data['City']=='Vancouver')]\n",
    "exercise2b #the above filters on multiple columns too but is a more complex code since there are multiple conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bd99f",
   "metadata": {},
   "source": [
    "Get Dummies for a Particular Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variable_1 = pd.get_dummies(df[\"fuel-type\"])\n",
    "\n",
    "'''you can then use the join a column code to join the dummy columns'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90e5721",
   "metadata": {},
   "source": [
    "Get Dummies for Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dfa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.get_dummies(data=df, columns = ['Airline', 'Source', 'Destination'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729602ae",
   "metadata": {},
   "source": [
    "For Label Encoding (Ordinal Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d5c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.replace({\"non-stop\":0,\"1 stop\":1,\"2 stops\":2,\"3 stops\":3,\"4 stops\":4},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd3c31",
   "metadata": {},
   "source": [
    "# For Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ec73a",
   "metadata": {},
   "source": [
    "Getting Correlation for all Features in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ebf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()\n",
    "\n",
    "df[['bore','stroke','compression-ratio','horsepower']].corr() '''for certain features'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e08462",
   "metadata": {},
   "source": [
    "To plot Regression for X(feature) and Y(target) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6844c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"engine-size\", y=\"price\", data=df)\n",
    "plt.ylim(0,) '''plt.ylim makes it look better if the data range starts from 0'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9de9189",
   "metadata": {},
   "source": [
    "To Plot Boxplot for a Single Column (Used for Univariate Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7864cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['Lot Area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66163f28",
   "metadata": {},
   "source": [
    "To plot Boxplots for Categorical Variables and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"body-style\", y=\"price\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc681c2",
   "metadata": {},
   "source": [
    "To Plot Scatterplot for Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248be72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_area = housing.plot.scatter(x='Gr Liv Area',\n",
    "                      y='SalePrice')\n",
    "\n",
    "outliers_dropped = housing.drop(housing.index[[1499,2181]]) #This was pasted here for removing outliers by indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d407cd",
   "metadata": {},
   "source": [
    "To get Statistics for Only Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fb3b3b",
   "metadata": {},
   "source": [
    "To get Unique Values for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['drive-wheels'].value_counts().to_frame()\n",
    "\n",
    "'''note that value counts work on Pandas series not dataframe, hence the single bracket'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a616bf2",
   "metadata": {},
   "source": [
    "To Create Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9a9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "New Codes:\n",
    "    \n",
    "data.groupby('species').mean() #the mean calculation\n",
    "\n",
    "data.groupby('species').median() #the median calculation\n",
    "\n",
    "\n",
    "Old Codes: \n",
    "\n",
    "df_gptest = df[['drive-wheels','body-style','price']]\n",
    "grouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()\n",
    "\n",
    "'''this gets the average for the categorical variables'''\n",
    "\n",
    "grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')\n",
    "grouped_pivot\n",
    "\n",
    "'''Converts the table to a pivot. Index is for rows and columns is for columns'''\n",
    "\n",
    "\n",
    "grouped_pivot = grouped_pivot.fillna(0)\n",
    "\n",
    "'''fill NaN values with 0'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248eff8d",
   "metadata": {},
   "source": [
    "To Calculate P-Value Between X and Y Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])\n",
    "print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97291683",
   "metadata": {},
   "source": [
    "To Construct Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2be60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data1.columns #to check all the columns in the dataset\n",
    "\n",
    "new_data = data1.loc[:,['Total_Stops', 'Airline_Air Asia',\n",
    "       'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
    "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_SpiceJet',\n",
    "       'Airline_Trujet', 'Airline_Vistara', 'Source_Banglore',\n",
    "       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n",
    "       'Destination_Banglore', 'Destination_Cochin', 'Destination_Delhi',\n",
    "       'Destination_Hyderabad', 'Destination_Kolkata', 'Destination_New Delhi',\n",
    "       'Duration_hours', 'Duration_minutes', 'Duration_Total_mins', 'Dep_Hour',\n",
    "       'Dep_Min', 'dep_timezone', 'Price']] #to take all the columns you want to check correlation for\n",
    "\n",
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(new_data.corr(),annot=True,cmap='RdYlGn')\n",
    "\n",
    "plt.show() #to draw the correlation heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376576c2",
   "metadata": {},
   "source": [
    "# For Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc10cc",
   "metadata": {},
   "source": [
    "Getting Correlations for Numerical Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "hous_num = housing.select_dtypes(include = ['float64', 'int64'])\n",
    "hous_num_corr = hous_num.corr()['SalePrice'][:-1] # -1 means that the latest row is SalePrice\n",
    "top_features = hous_num_corr[abs(hous_num_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n",
    "print(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(top_features), top_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4a5c56",
   "metadata": {},
   "source": [
    "Plotting a Histogram for Skewed Target Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce599ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_untransformed = sns.distplot(housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25070f56",
   "metadata": {},
   "source": [
    "Getting Skewness in Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % housing['SalePrice'].skew()) \n",
    "\n",
    "'''between -0.5 and 0.5 is a normal distribution.  \n",
    "moderate skewness is -0.5 to -1.0 and 0.5 to 1.0. highly skewed distribution is < -1.0 and > 1.0'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d76f3a",
   "metadata": {},
   "source": [
    "Performing Normal Test for a Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import normaltest # D'Agostino K^2 Test\n",
    "\n",
    "normaltest(df.target_column.values) #Frequentist statisticians say if p-value is > 0.05, the distribution is normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b59c1a",
   "metadata": {},
   "source": [
    "Making a Distribution Normal After Performing Normal Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee422e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_medv = np.log(df.target_column) #For Log Transformation\n",
    "\n",
    "normaltest(log_medv) #to check if it is now normal\n",
    "\n",
    "\n",
    "sqrt_medv = np.sqrt(df.target_column) #For Square Root transformation\n",
    "normaltest(sqrt_medv) #to check if it is now normal\n",
    "\n",
    "\n",
    "FOR BOXCOX TRANSFORMATION\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "bc_result = boxcox(df.target_column)\n",
    "boxcox_medv = bc_result[0] #this is the dataframe itself\n",
    "lam = bc_result[1] #this is the lambda value to reverse the boxcox transformation\n",
    "\n",
    "normaltest(boxcox_medv) #to check if it is now normal\n",
    "\n",
    "from scipy.special import inv_boxcox #To perform inverse boxcox\n",
    "inv_boxcox(boxcox_medv, lam) #to revert the values\n",
    "\n",
    "inv_boxcox(boxcox_medv, lam)[:10] #you can use this to compare the first 10 values with the original y variable\n",
    "\n",
    "'''after doing inverse boxcox you can then get an r2 score'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18850b54",
   "metadata": {},
   "source": [
    "Transforming Target Variable with np.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cea965",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log(housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aed975",
   "metadata": {},
   "source": [
    "Using Standard Scaler for Float and Integer Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c354e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_data = StandardScaler().fit_transform(hous_num)\n",
    "scaled_data\n",
    "\n",
    "'''note that hous_num is a dataset of numerical values only'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bafbca",
   "metadata": {},
   "source": [
    "# For Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698017af",
   "metadata": {},
   "source": [
    "Importing Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64313ec4",
   "metadata": {},
   "source": [
    "Creating a Linear Regression Object and a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaee96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "X = df[['highway-mpg']]\n",
    "Y = df['price']\n",
    "lm.fit(X,Y)\n",
    "\n",
    "'''for simple linear regression models'''\n",
    "\n",
    "Z = df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']]\n",
    "lm.fit(Z, df['price'])\n",
    "\n",
    "'''for multiple linear regression models'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3119fa9",
   "metadata": {},
   "source": [
    "Predicting Values on a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat=lm.predict(X)\n",
    "Yhat[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec3cad",
   "metadata": {},
   "source": [
    "Getting the Intercept, Slope, and R-Squared Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.intercept_\n",
    "lm.coef_\n",
    "lm.score (x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548c3ea7",
   "metadata": {},
   "source": [
    "To Visualize a Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9295c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "\n",
    "ax1 = sns.distplot(df['price'], hist=False, color=\"r\", label=\"Actual Value\")\n",
    "sns.distplot(Y_hat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n",
    "\n",
    "\n",
    "plt.title('Actual vs Fitted Values for Price')\n",
    "plt.xlabel('Price (in dollars)')\n",
    "plt.ylabel('Proportion of Cars')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "'''to plot distribution plot for MLR'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d146bbe",
   "metadata": {},
   "source": [
    "To Create a Polynomial Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a5bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]\n",
    "pipe=Pipeline(Input)\n",
    "\n",
    "pipe.fit(Z,y)\n",
    "\n",
    "ypipe=pipe.predict(Z)\n",
    "ypipe[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc6230",
   "metadata": {},
   "source": [
    "Creating Polynomial Features, Scaling, and Creating/Getting a Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9224728",
   "metadata": {},
   "outputs": [],
   "source": [
    "New Code: \n",
    "pf = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_pf = pf.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pf, y, test_size=0.3, \n",
    "                                                    random_state=72018)\n",
    "\n",
    "\n",
    "s = StandardScaler()\n",
    "X_train_s = s.fit_transform(X_train)\n",
    "\n",
    "lr.fit(X_train_s, y_train)\n",
    "X_test_s = s.transform(X_test)\n",
    "y_pred_bc = lr.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740aba1",
   "metadata": {},
   "source": [
    "To Visualize a Polynomial Regressional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPolly(model, independent_variable, dependent_variabble, Name):\n",
    "    x_new = np.linspace(15, 55, 100)\n",
    "    y_new = model(x_new)\n",
    "\n",
    "    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')\n",
    "    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor((0.898, 0.898, 0.898))\n",
    "    fig = plt.gcf()\n",
    "    plt.xlabel(Name)\n",
    "    plt.ylabel('Price of Cars')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    '''Note that the name argument in the function is the label for the x axis. The model is the polynomial model variable'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ac454",
   "metadata": {},
   "source": [
    "To Get Mean Squared Error for Linear Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5dd0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Yhat=lm.predict(X)\n",
    "mse = mean_squared_error(df['price'], Yhat)\n",
    "print('The mean square error of price and predicted value is: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284fe05",
   "metadata": {},
   "source": [
    "# For Model Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2160ca",
   "metadata": {},
   "source": [
    "For Splitting Into Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.10, random_state=1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lre=LinearRegression()\n",
    "lre.fit(x_train[['horsepower']], y_train)\n",
    "lre.score(x_test[['horsepower']], y_test)\n",
    "\n",
    "'''model is saved to the lre object and used on the test data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19866021",
   "metadata": {},
   "source": [
    "   ###         To understand this new Polynomial Regression Code better, you need to realize that the train and test features are first transformed to a Polynomial dimension. Then the Linear Regression Object is used to fit the transformed features to create a model\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321441f2",
   "metadata": {},
   "source": [
    "Create Polynomial Regression Model Using xTrain and xTest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f497eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PolynomialFeatures(degree=5)\n",
    "x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "x_test_pr = pr.fit_transform(x_test[['horsepower']])\n",
    "\n",
    "poly = LinearRegression()\n",
    "poly.fit(x_train_pr, y_train)\n",
    "poly.score(x_train_pr, y_train)\n",
    "\n",
    "'''a great idea is to first see plot of best polynomial order before creating the model'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd3e17",
   "metadata": {},
   "source": [
    "To See Plot of Best Polynomial Order with R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "\n",
    "order = [1, 2, 3, 4]\n",
    "for n in order:\n",
    "    pr = PolynomialFeatures(degree=n)\n",
    "    \n",
    "    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n",
    "    \n",
    "    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n",
    "    \n",
    "    lr.fit(x_train_pr, y_train)\n",
    "    \n",
    "    Rsqu_test.append(lr.score(x_test_pr, y_test))\n",
    "\n",
    "plt.plot(order, Rsqu_test)\n",
    "plt.xlabel('order')\n",
    "plt.ylabel('R^2')\n",
    "plt.title('R^2 Using Test Data')\n",
    "plt.text(3, 0.75, 'Maximum R^2 ')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210f43d",
   "metadata": {},
   "source": [
    "To Create kFold Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(shuffle=True, random_state=72018, n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be80c81",
   "metadata": {},
   "source": [
    "Using Cross Val Predict on Pipeline and Kfold Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(estimator, X, y, cv=kf) #estimator is a Pipeline object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b98e2",
   "metadata": {},
   "source": [
    "Creating a Pipeline with Polynomial Features, Scaler, and Regularization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = Pipeline([\n",
    "                    (\"scaler\", s),\n",
    "                    (\"make_higher_degree\", PolynomialFeatures(degree=2)),\n",
    "                    (\"lasso_regression\", Lasso(alpha=0.03))])\n",
    "\n",
    "best_estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013c19d",
   "metadata": {},
   "source": [
    "To Calculate Cross Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e80b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)\n",
    "print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abedd08",
   "metadata": {},
   "source": [
    "To Run a Grid Search and Get a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "parameters1= [{'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 100000]}]\n",
    "\n",
    "RR=Ridge()\n",
    "RR\n",
    "\n",
    "Grid1 = GridSearchCV(RR, parameters1,cv=4)\n",
    "Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)\n",
    "\n",
    "BestRR=Grid1.best_estimator_\n",
    "BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4731733",
   "metadata": {},
   "source": [
    "To Create Root Mean Squared Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def rmse(ytrue, ypredicted):\n",
    "    return np.sqrt(mean_squared_error(ytrue, ypredicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6597bf",
   "metadata": {},
   "source": [
    "Using RidgeCV for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n",
    "\n",
    "ridgeCV = RidgeCV(alphas=alphas, \n",
    "                  cv=4).fit(X_train, y_train)\n",
    "\n",
    "ridgeCV_rmse = rmse(y_test, ridgeCV.predict(X_test)) #or MSE\n",
    "\n",
    "print(ridgeCV.alpha_, ridgeCV_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9da1d",
   "metadata": {},
   "source": [
    "Using LassoCV for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005, 0.005, 0.05, 0.1, 1, 5, 20, 50, 80, 100, 120, 150])\n",
    "\n",
    "lassoCV = LassoCV(alphas=alphas2,\n",
    "                  max_iter=5e4,\n",
    "                  cv=3).fit(X_train, y_train)\n",
    "\n",
    "lassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))\n",
    "\n",
    "print(lassoCV.alpha_, lassoCV_rmse)  # Lasso is slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804b301",
   "metadata": {},
   "source": [
    "Using ElasticNetCV for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "l1_ratios = np.linspace(0.1, 0.9, 9)\n",
    "\n",
    "elasticNetCV = ElasticNetCV(alphas=alphas2, \n",
    "                            l1_ratio=l1_ratios,\n",
    "                            max_iter=1e4).fit(X_train, y_train)\n",
    "elasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))\n",
    "\n",
    "print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e26575",
   "metadata": {},
   "source": [
    "Comparing RMSE from Regularization CV Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8781b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_vals = [linearRegression_rmse, ridgeCV_rmse, lassoCV_rmse, elasticNetCV_rmse]\n",
    "\n",
    "labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n",
    "\n",
    "rmse_df = pd.Series(rmse_vals, index=labels).to_frame()\n",
    "rmse_df.rename(columns={0: 'RMSE'}, inplace=1)\n",
    "rmse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36131d3",
   "metadata": {},
   "source": [
    "## Typical Libraries to be Imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
